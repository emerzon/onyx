From a109a03cb88df62b572d5232e780402d6afc3b1a Mon Sep 17 00:00:00 2001
From: Emerson Gomes <emerson.gomes@thalesgroup.com>
Date: Sat, 5 Jul 2025 16:13:43 -0500
Subject: [PATCH] Add Token Counter feature

---
 ...2e26b80_add_token_usage_to_chat_message.py |  26 +++++
 backend/onyx/chat/models.py                   |  23 +++-
 backend/onyx/chat/process_message.py          |  84 ++++++++++++++-
 backend/onyx/chat/token_usage_tracker.py      |  83 +++++++++++++++
 backend/onyx/db/chat.py                       |   6 +-
 backend/onyx/db/models.py                     |   2 +
 backend/onyx/llm/chat_llm.py                  |  28 +++++
 backend/onyx/server/query_and_chat/models.py  |   1 +
 web/src/app/chat/ChatPage.tsx                 |  22 ++++
 web/src/app/chat/input/ChatInputBar.tsx       |  43 +++++++-
 web/src/app/chat/interfaces.ts                |  17 +++
 web/src/app/chat/lib.tsx                      |   4 +-
 web/src/components/chat/TokenCounter.tsx      | 100 ++++++++++++++++++
 web/src/hooks/useTokenCounter.ts              |  71 +++++++++++++
 14 files changed, 500 insertions(+), 10 deletions(-)
 create mode 100644 backend/alembic/versions/42e26b80_add_token_usage_to_chat_message.py
 create mode 100644 backend/onyx/chat/token_usage_tracker.py
 create mode 100644 web/src/components/chat/TokenCounter.tsx
 create mode 100644 web/src/hooks/useTokenCounter.ts

diff --git a/backend/alembic/versions/42e26b80_add_token_usage_to_chat_message.py b/backend/alembic/versions/42e26b80_add_token_usage_to_chat_message.py
new file mode 100644
index 000000000..1933fe51b
--- /dev/null
+++ b/backend/alembic/versions/42e26b80_add_token_usage_to_chat_message.py
@@ -0,0 +1,26 @@
+"""add token_usage to chat_message
+
+Revision ID: 42e26b80
+Revises: 58c50ef19f08
+Create Date: 2025-07-05 16:30:00.000000
+
+"""
+from alembic import op
+import sqlalchemy as sa
+from sqlalchemy.dialects import postgresql
+
+# revision identifiers, used by Alembic.
+revision = '42e26b80'
+down_revision = '58c50ef19f08'
+branch_labels = None
+depends_on = None
+
+
+def upgrade() -> None:
+    # Add token_usage column to chat_message table
+    op.add_column('chat_message', sa.Column('token_usage', postgresql.JSONB(), nullable=True))
+
+
+def downgrade() -> None:
+    # Remove token_usage column from chat_message table
+    op.drop_column('chat_message', 'token_usage')
\ No newline at end of file
diff --git a/backend/onyx/chat/models.py b/backend/onyx/chat/models.py
index 4b3b4b0a3..97b66e9b3 100644
--- a/backend/onyx/chat/models.py
+++ b/backend/onyx/chat/models.py
@@ -131,6 +131,11 @@ class UserKnowledgeFilePacket(BaseModel):
     user_files: list[FileDescriptor]
 
 
+class TokenUsagePacket(BaseModel):
+    """Packet containing token usage information from LLM responses"""
+    token_usage: dict[str, Any]
+
+
 class LLMRelevanceFilterResponse(BaseModel):
     llm_selected_doc_indices: list[int]
 
@@ -260,6 +265,18 @@ class PersonaOverrideConfig(BaseModel):
     custom_tools_openapi: list[dict[str, Any]] = Field(default_factory=list)
 
 
+class LLMMetricsContainer(BaseModel):
+    prompt_tokens: int
+    response_tokens: int
+
+
+class TokenUsageResponse(BaseModel):
+    """Token usage information for LLM API responses"""
+    usage: dict[str, Union[int, dict[str, int]]] = Field(
+        description="Token usage data with prompt_tokens, completion_tokens, total_tokens, and optional completion_tokens_details"
+    )
+
+
 AnswerQuestionPossibleReturn = (
     OnyxAnswerPiece
     | CitationInfo
@@ -267,17 +284,13 @@ AnswerQuestionPossibleReturn = (
     | CustomToolResponse
     | StreamingError
     | StreamStopInfo
+    | TokenUsageResponse
 )
 
 
 AnswerQuestionStreamReturn = Iterator[AnswerQuestionPossibleReturn]
 
 
-class LLMMetricsContainer(BaseModel):
-    prompt_tokens: int
-    response_tokens: int
-
-
 StreamProcessor = Callable[[Iterator[str]], AnswerQuestionStreamReturn]
 
 
diff --git a/backend/onyx/chat/process_message.py b/backend/onyx/chat/process_message.py
index fa67bff45..9ee00daca 100644
--- a/backend/onyx/chat/process_message.py
+++ b/backend/onyx/chat/process_message.py
@@ -4,6 +4,7 @@ from collections import defaultdict
 from collections.abc import Callable
 from collections.abc import Generator
 from collections.abc import Iterator
+from typing import Any
 from typing import cast
 from typing import Protocol
 from uuid import UUID
@@ -40,7 +41,9 @@ from onyx.chat.models import StreamingError
 from onyx.chat.models import StreamStopInfo
 from onyx.chat.models import StreamStopReason
 from onyx.chat.models import SubQuestionKey
+from onyx.chat.models import TokenUsagePacket
 from onyx.chat.models import UserKnowledgeFilePacket
+from onyx.chat.token_usage_tracker import token_usage_tracker
 from onyx.chat.prompt_builder.answer_prompt_builder import AnswerPromptBuilder
 from onyx.chat.prompt_builder.answer_prompt_builder import default_build_system_message
 from onyx.chat.prompt_builder.answer_prompt_builder import default_build_user_message
@@ -409,6 +412,7 @@ ChatPacket = (
     | StreamStopInfo
     | AgentSearchPacket
     | UserKnowledgeFilePacket
+    | TokenUsagePacket
 )
 ChatPacketStream = Iterator[ChatPacket]
 
@@ -849,6 +853,10 @@ def stream_chat_message_objects(
             error: str | None,
             tool_call: ToolCall | None,
         ) -> ChatMessage:
+            # Get current token usage at time of message creation
+            usage_data = token_usage_tracker.get_usage()
+            logger.info(f"Token usage in create_new_chat_message: {usage_data}")
+            
             return create_new_chat_message(
                 chat_session_id=chat_session_id,
                 parent_message=(
@@ -872,7 +880,9 @@ def stream_chat_message_objects(
                 commit=False,
                 reserved_message_id=reserved_message_id,
                 is_agentic=new_msg_req.use_agentic_search,
+                token_usage=usage_data,
             )
+        
 
         partial_response = create_response
 
@@ -1021,7 +1031,12 @@ def stream_chat_message_objects(
             lambda: AnswerPostInfo(ai_message_files=[])
         )
         refined_answer_improvement = True
+        
+        # Variable to store token usage captured during streaming
+        streaming_captured_token_usage = None
+        
         for packet in answer.processed_streamed_output:
+            
             if isinstance(packet, ToolResponse):
                 info_by_subq = yield from _process_tool_response(
                     packet=packet,
@@ -1052,6 +1067,22 @@ def stream_chat_message_objects(
                     ]
                     info.tool_result = packet
                 yield cast(ChatPacket, packet)
+            
+            # Capture token usage during streaming - check after each packet
+            # Try thread-local first, then fall back to latest global usage
+            current_usage = token_usage_tracker.get_usage() or token_usage_tracker.get_latest_usage()
+            if current_usage is not None and streaming_captured_token_usage is None:
+                streaming_captured_token_usage = current_usage
+                logger.info(f"Token usage captured during streaming: {streaming_captured_token_usage}")
+                # Don't yield TokenUsagePacket - token usage will only be in the final message
+
+        # Capture token usage immediately after streaming is complete
+        immediate_token_usage = token_usage_tracker.get_usage() or token_usage_tracker.get_latest_usage()
+        logger.info(f"Token usage captured immediately after streaming: {immediate_token_usage}")
+        
+        # Use the best available token usage (streaming capture takes precedence)
+        final_token_usage = streaming_captured_token_usage or immediate_token_usage
+        logger.info(f"Final token usage to use: {final_token_usage}")
 
     except ValueError as e:
         logger.exception("Failed to process chat message.")
@@ -1087,6 +1118,9 @@ def stream_chat_message_objects(
         db_session.rollback()
         return
 
+    # Use the token usage captured during or after streaming
+    logger.info(f"Passing final token usage to post-processing: {final_token_usage}")
+
     yield from _post_llm_answer_processing(
         answer=answer,
         info_by_subq=info_by_subq,
@@ -1096,6 +1130,7 @@ def stream_chat_message_objects(
         db_session=db_session,
         chat_session_id=chat_session_id,
         refined_answer_improvement=refined_answer_improvement,
+        captured_token_usage=final_token_usage,
     )
 
 
@@ -1108,6 +1143,7 @@ def _post_llm_answer_processing(
     db_session: Session,
     chat_session_id: UUID,
     refined_answer_improvement: bool | None,
+    captured_token_usage: dict[str, Any] | None = None,
 ) -> Generator[ChatPacket, None, None]:
     """
     Stores messages in the db and yields some final packets to the frontend
@@ -1146,7 +1182,40 @@ def _post_llm_answer_processing(
                 )
             ]
         )
-        gen_ai_response_message = partial_response(
+        
+        # Create a wrapper function that overrides token usage for the final message
+        def create_final_message_with_captured_usage(
+            message: str,
+            rephrased_query: str | None,
+            reference_docs: list[DbSearchDoc] | None,
+            files: list[FileDescriptor],
+            token_count: int,
+            citations: dict[int, int] | None,
+            error: str | None,
+            tool_call: ToolCall | None,
+        ) -> ChatMessage:
+            # Call the original partial_response function but override the stored usage
+            msg = partial_response(
+                message=message,
+                rephrased_query=rephrased_query,
+                reference_docs=reference_docs,
+                files=files,
+                token_count=token_count,
+                citations=citations,
+                error=error,
+                tool_call=tool_call,
+            )
+            
+            # Override the token_usage with our captured value
+            if captured_token_usage is not None:
+                logger.info(f"Overriding token_usage in final message with captured value: {captured_token_usage}")
+                msg.token_usage = captured_token_usage
+            else:
+                logger.warning("No captured token usage available for final message")
+            
+            return msg
+            
+        gen_ai_response_message = create_final_message_with_captured_usage(
             message=answer.llm_answer,
             rephrased_query=(
                 info.qa_docs_response.rephrased_query if info.qa_docs_response else None
@@ -1209,9 +1278,17 @@ def _post_llm_answer_processing(
                     else None
                 ),
                 error=ERROR_TYPE_CANCELLED if answer.is_cancelled() else None,
+                token_usage=token_usage_tracker.get_usage(),
                 refined_answer_improvement=refined_answer_improvement,
                 is_agentic=True,
             )
+            
+            # Debug logging
+            usage_data = token_usage_tracker.get_usage()
+            logger.info(f"Token usage being stored in message: {usage_data}")
+            
+            # Clear token usage after storing it in the message
+            token_usage_tracker.clear_usage()
             agentic_message_ids.append(
                 AgentMessageIDInfo(level=next_level, message_id=next_answer_message.id)
             )
@@ -1225,6 +1302,7 @@ def _post_llm_answer_processing(
 
         yield AgenticMessageResponseIDInfo(agentic_message_ids=agentic_message_ids)
 
+        logger.info(f"Final message before yield - token_usage: {gen_ai_response_message.token_usage}")
         yield translate_db_message_to_chat_message_detail(gen_ai_response_message)
     except Exception as e:
         error_msg = str(e)
@@ -1232,6 +1310,10 @@ def _post_llm_answer_processing(
 
         # Frontend will erase whatever answer and show this instead
         yield StreamingError(error="Failed to parse LLM output")
+    
+    finally:
+        # Clean up token usage tracker for this request
+        token_usage_tracker.clear_usage()
 
 
 @log_generator_function_time()
diff --git a/backend/onyx/chat/token_usage_tracker.py b/backend/onyx/chat/token_usage_tracker.py
new file mode 100644
index 000000000..320edf1ae
--- /dev/null
+++ b/backend/onyx/chat/token_usage_tracker.py
@@ -0,0 +1,83 @@
+"""
+Token usage tracking utility for capturing LLM token consumption
+across the chat processing pipeline.
+"""
+from typing import Dict, Any, Optional
+from threading import local
+import threading
+import time
+
+
+class TokenUsageTracker:
+    """Thread-local storage for tracking token usage during chat processing."""
+    
+    def __init__(self):
+        self._local = local()
+        # Non-thread-local storage for cross-thread access
+        self._global_storage = {}
+        self._storage_lock = threading.Lock()
+    
+    def set_usage(self, usage_data: Dict[str, Any]) -> None:
+        """Store token usage data for the current thread/request."""
+        # Don't overwrite existing valid usage data with None/empty data
+        # This prevents tool selection LLM calls from clearing the main chat LLM usage
+        current_usage = getattr(self._local, 'usage_data', None)
+        if current_usage is not None and usage_data is None:
+            return
+            
+        # Don't overwrite larger token usage with smaller usage
+        # This prevents tool selection LLM calls from overwriting main chat LLM usage
+        if (current_usage is not None and usage_data is not None and
+            'total_tokens' in current_usage and 'total_tokens' in usage_data):
+            if usage_data['total_tokens'] < current_usage['total_tokens']:
+                from onyx.utils.logger import setup_logger
+                logger = setup_logger()
+                logger.info(f"[TOKEN_TRACKER] Preventing overwrite of larger usage {current_usage['total_tokens']} with smaller usage {usage_data['total_tokens']}")
+                return
+            
+        self._local.usage_data = usage_data
+        
+        # Also store globally with a timestamp for cross-thread access
+        current_time = time.time()
+        thread_id = threading.get_ident()
+        key = f"{thread_id}_{current_time}"
+        
+        with self._storage_lock:
+            # Store with timestamp for cleanup
+            self._global_storage[key] = {
+                'usage_data': usage_data, 
+                'timestamp': current_time,
+                'thread_id': thread_id
+            }
+            # Clean up old entries (older than 60 seconds)
+            cutoff_time = current_time - 60
+            keys_to_remove = [k for k, v in self._global_storage.items() if v['timestamp'] < cutoff_time]
+            for k in keys_to_remove:
+                del self._global_storage[k]
+    
+    def get_usage(self) -> Optional[Dict[str, Any]]:
+        """Retrieve token usage data for the current thread/request."""
+        return getattr(self._local, 'usage_data', None)
+    
+    def get_latest_usage(self) -> Optional[Dict[str, Any]]:
+        """Get the most recent token usage from any thread."""
+        with self._storage_lock:
+            if not self._global_storage:
+                return None
+            
+            # Find the most recent entry
+            latest_entry = max(self._global_storage.values(), key=lambda x: x['timestamp'])
+            return latest_entry['usage_data']
+    
+    def clear_usage(self) -> None:
+        """Clear token usage data for the current thread/request."""
+        if hasattr(self._local, 'usage_data'):
+            delattr(self._local, 'usage_data')
+    
+    def has_usage(self) -> bool:
+        """Check if token usage data is available."""
+        return hasattr(self._local, 'usage_data') and self._local.usage_data is not None
+
+
+# Global instance for tracking token usage
+token_usage_tracker = TokenUsageTracker()
\ No newline at end of file
diff --git a/backend/onyx/db/chat.py b/backend/onyx/db/chat.py
index c2a790c68..e13832beb 100644
--- a/backend/onyx/db/chat.py
+++ b/backend/onyx/db/chat.py
@@ -647,6 +647,7 @@ def create_new_chat_message(
     overridden_model: str | None = None,
     refined_answer_improvement: bool | None = None,
     is_agentic: bool = False,
+    token_usage: dict[str, Any] | None = None,
 ) -> ChatMessage:
     if reserved_message_id is not None:
         # Edit existing message
@@ -669,6 +670,7 @@ def create_new_chat_message(
         existing_message.overridden_model = overridden_model
         existing_message.refined_answer_improvement = refined_answer_improvement
         existing_message.is_agentic = is_agentic
+        existing_message.token_usage = token_usage
         new_chat_message = existing_message
     else:
         # Create new message
@@ -689,6 +691,7 @@ def create_new_chat_message(
             overridden_model=overridden_model,
             refined_answer_improvement=refined_answer_improvement,
             is_agentic=is_agentic,
+            token_usage=token_usage,
         )
         db_session.add(new_chat_message)
 
@@ -1067,8 +1070,9 @@ def translate_db_message_to_chat_message_detail(
         refined_answer_improvement=chat_message.refined_answer_improvement,
         is_agentic=chat_message.is_agentic,
         error=chat_message.error,
+        token_usage=chat_message.token_usage,
     )
-
+    
     return chat_msg_detail
 
 
diff --git a/backend/onyx/db/models.py b/backend/onyx/db/models.py
index 10108a24a..6f705f326 100644
--- a/backend/onyx/db/models.py
+++ b/backend/onyx/db/models.py
@@ -1989,6 +1989,8 @@ class ChatMessage(Base):
 
     is_agentic: Mapped[bool] = mapped_column(Boolean, default=False)
     refined_answer_improvement: Mapped[bool] = mapped_column(Boolean, nullable=True)
+    # Token usage information from LLM API responses
+    token_usage: Mapped[dict[str, Any] | None] = mapped_column(postgresql.JSONB(), nullable=True)
 
     chat_session: Mapped[ChatSession] = relationship("ChatSession")
     prompt: Mapped[Optional["Prompt"]] = relationship("Prompt")
diff --git a/backend/onyx/llm/chat_llm.py b/backend/onyx/llm/chat_llm.py
index e79aeb9f4..89eafbbef 100644
--- a/backend/onyx/llm/chat_llm.py
+++ b/backend/onyx/llm/chat_llm.py
@@ -46,6 +46,7 @@ from onyx.llm.llm_provider_options import CREDENTIALS_FILE_CUSTOM_CONFIG_KEY
 from onyx.llm.utils import model_is_reasoning_model
 from onyx.server.utils import mask_string
 from onyx.utils.logger import setup_logger
+from onyx.chat.token_usage_tracker import token_usage_tracker
 from onyx.utils.long_term_log import LongTermLogger
 
 
@@ -399,6 +400,8 @@ class DefaultMultiLLM(LLM):
                 "max_tokens": max_tokens,
                 # streaming choice
                 "stream": stream,
+                # Enable usage tracking for streaming
+                **({"stream_options": {"include_usage": True}} if stream else {}),
                 # model params
                 "temperature": self._temperature,
                 "timeout": timeout_override or self._timeout,
@@ -500,6 +503,17 @@ class DefaultMultiLLM(LLM):
         )
         choice = response.choices[0]
         message = getattr(choice, "message", None)
+        
+        # Capture token usage from non-streaming response
+        if hasattr(response, 'usage') and response.usage:
+            # Convert usage to dict format if needed
+            usage_data = response.usage
+            if hasattr(usage_data, 'model_dump'):
+                usage_data = usage_data.model_dump()
+            elif hasattr(usage_data, '__dict__'):
+                usage_data = usage_data.__dict__
+            token_usage_tracker.set_usage(usage_data)
+        
         if message:
             output = _convert_litellm_message_to_langchain_message(message)
             if output:
@@ -557,6 +571,19 @@ class DefaultMultiLLM(LLM):
                     stop_reason=choice["finish_reason"],
                 )
 
+                # Capture token usage information when available
+                if "usage" in part and part["usage"]:
+                    # Convert usage to dict format if needed
+                    usage_data = part["usage"]
+                    if hasattr(usage_data, 'model_dump'):
+                        usage_data = usage_data.model_dump()
+                    elif hasattr(usage_data, 'dict'):
+                        usage_data = usage_data.dict()
+                    elif hasattr(usage_data, '__dict__'):
+                        usage_data = usage_data.__dict__
+                    # Store in thread-local tracker for access by chat processing
+                    token_usage_tracker.set_usage(usage_data)
+
                 if output is None:
                     output = message_chunk
                 else:
@@ -569,6 +596,7 @@ class DefaultMultiLLM(LLM):
                 "The AI model failed partway through generation, please try again."
             )
 
+
         if output:
             self._record_result(prompt, output)
 
diff --git a/backend/onyx/server/query_and_chat/models.py b/backend/onyx/server/query_and_chat/models.py
index 56cedc212..618cadbfd 100644
--- a/backend/onyx/server/query_and_chat/models.py
+++ b/backend/onyx/server/query_and_chat/models.py
@@ -246,6 +246,7 @@ class ChatMessageDetail(BaseModel):
     refined_answer_improvement: bool | None = None
     is_agentic: bool | None = None
     error: str | None = None
+    token_usage: dict[str, Any] | None = None
 
     def model_dump(self, *args: list, **kwargs: dict[str, Any]) -> dict[str, Any]:  # type: ignore
         initial_dict = super().model_dump(mode="json", *args, **kwargs)  # type: ignore
diff --git a/web/src/app/chat/ChatPage.tsx b/web/src/app/chat/ChatPage.tsx
index 12aeca645..19ec8f909 100644
--- a/web/src/app/chat/ChatPage.tsx
+++ b/web/src/app/chat/ChatPage.tsx
@@ -97,6 +97,8 @@ import {
 } from "@/lib/llm/utils";
 import { ChatInputBar } from "./input/ChatInputBar";
 import { useChatContext } from "@/components/context/ChatContext";
+import { useTokenCounter } from "@/hooks/useTokenCounter";
+import { TokenCounter } from "@/components/chat/TokenCounter";
 import { ChatPopup } from "./ChatPopup";
 import FunctionalHeader from "@/components/chat/Header";
 import { useSidebarVisibility } from "@/components/chat/hooks";
@@ -191,6 +193,8 @@ export function ChatPage({
     setCurrentMessageFiles,
   } = useDocumentsContext();
 
+  const { sessionUsage, updateTokenUsage, resetTokenCounter } = useTokenCounter();
+
   const defaultAssistantIdRaw = searchParams?.get(
     SEARCH_PARAM_NAMES.PERSONA_ID
   );
@@ -448,6 +452,8 @@ export function ChatPage({
     const isChatSessionSwitch = existingChatSessionId !== priorChatSessionId;
     if (isChatSessionSwitch) {
       // de-select documents
+      // reset token counter when switching sessions
+      resetTokenCounter();
 
       // reset all filters
       filterManager.setSelectedDocumentSets([]);
@@ -519,6 +525,14 @@ export function ChatPage({
         updateCompleteMessageDetail(chatSession.chat_session_id, newMessageMap);
       }
 
+      // Initialize token counter with existing message token usage
+      // Note: Only messages created after token usage implementation will have usage data
+      newMessageHistory.forEach((message) => {
+        if (message.type === "assistant" && message.token_usage) {
+          updateTokenUsage(message.token_usage);
+        }
+      });
+
       setChatSessionSharedStatus(chatSession.shared_status);
 
       // go to bottom. If initial load, then do a scroll,
@@ -1920,6 +1934,12 @@ export function ChatPage({
         }
       }
     }
+    
+    // Handle token usage from final message for all cases (new session and existing session)
+    if (finalMessage?.token_usage) {
+      updateTokenUsage(finalMessage.token_usage);
+    }
+    
     if (
       finalMessage?.context_docs &&
       finalMessage.context_docs.top_documents.length > 0 &&
@@ -3409,6 +3429,8 @@ export function ChatPage({
                               setFiles={setCurrentMessageFiles}
                               handleFileUpload={handleMessageSpecificFileUpload}
                               textAreaRef={textAreaRef}
+                              sessionUsage={sessionUsage}
+                              availableLlmProviders={llmProviders}
                             />
                             {enterpriseSettings &&
                               enterpriseSettings.custom_lower_disclaimer_content && (
diff --git a/web/src/app/chat/input/ChatInputBar.tsx b/web/src/app/chat/input/ChatInputBar.tsx
index e7cbdb99e..26ccfec7f 100644
--- a/web/src/app/chat/input/ChatInputBar.tsx
+++ b/web/src/app/chat/input/ChatInputBar.tsx
@@ -8,6 +8,7 @@ import { InputPrompt } from "@/app/chat/interfaces";
 
 import { FilterManager, getDisplayNameForModel, LlmManager } from "@/lib/hooks";
 import { useChatContext } from "@/components/context/ChatContext";
+import { LLMProviderDescriptor } from "@/app/admin/configuration/llm/interfaces";
 import { ChatFileType, FileDescriptor } from "../interfaces";
 import {
   DocumentIcon2,
@@ -41,6 +42,8 @@ import { SettingsContext } from "@/components/settings/SettingsProvider";
 import { getProviderIcon } from "@/app/admin/configuration/llm/utils";
 import { useDocumentsContext } from "../my-documents/DocumentsContext";
 import { UploadIntent } from "../ChatPage";
+import { TokenCounter } from "@/components/chat/TokenCounter";
+import { SessionTokenUsage } from "@/hooks/useTokenCounter";
 
 const MAX_INPUT_HEIGHT = 200;
 export const SourceChip2 = ({
@@ -198,6 +201,8 @@ interface ChatInputBarProps {
   retrievalEnabled: boolean;
   proSearchEnabled: boolean;
   setProSearchEnabled: (proSearchEnabled: boolean) => void;
+  sessionUsage: SessionTokenUsage;
+  availableLlmProviders: LLMProviderDescriptor[]; // For getting correct model max tokens
 }
 
 export function ChatInputBar({
@@ -228,6 +233,8 @@ export function ChatInputBar({
   llmManager,
   proSearchEnabled,
   setProSearchEnabled,
+  sessionUsage,
+  availableLlmProviders,
 }: ChatInputBarProps) {
   const { user } = useUser();
   const {
@@ -847,7 +854,7 @@ export function ChatInputBar({
               </div>
             )}
 
-            <div className="flex pr-4 pb-2 justify-between bg-input-background items-center w-full ">
+            <div className="flex pr-4 pb-2 bg-input-background items-center w-full ">
               <div className="space-x-1 flex  px-4 ">
                 <ChatInputOption
                   flexPriority="stiff"
@@ -910,7 +917,39 @@ export function ChatInputBar({
                   />
                 )}
               </div>
-              <div className="flex items-center my-auto">
+              
+              {/* Center section with token counter */}
+              <div className="flex-1 flex justify-center">
+                <TokenCounter 
+                  sessionUsage={sessionUsage} 
+                  currentModel={{
+                    modelName: llmManager.currentLlm?.modelName || "Unknown Model",
+                    maxTokens: (() => {
+                      // Use the same pattern as ChatPage to get correct max_input_tokens
+                      const currentModelName = llmManager.currentLlm?.modelName;
+                      if (!currentModelName) return 200000;
+                      
+                      // Create model descriptors like ChatPage does
+                      const modelDescriptors = availableLlmProviders.flatMap((provider) =>
+                        provider.model_configurations.map((modelConfiguration) => ({
+                          modelName: modelConfiguration.name,
+                          provider: provider.provider,
+                          maxTokens: modelConfiguration.max_input_tokens!,
+                        }))
+                      );
+                      
+                      // Find the matching model configuration
+                      const matchingModel = modelDescriptors.find(
+                        (descriptor) => descriptor.modelName === currentModelName
+                      );
+                      
+                      return matchingModel?.maxTokens || 200000; // Fallback if not found
+                    })(),
+                  }}
+                />
+              </div>
+              
+              <div className="flex items-center my-auto gap-2">
                 {retrievalEnabled && settings?.settings.pro_search_enabled && (
                   <AgenticToggle
                     proSearchEnabled={proSearchEnabled}
diff --git a/web/src/app/chat/interfaces.ts b/web/src/app/chat/interfaces.ts
index a236290c9..4143eb3c6 100644
--- a/web/src/app/chat/interfaces.ts
+++ b/web/src/app/chat/interfaces.ts
@@ -110,6 +110,14 @@ export interface Message {
   stopReason?: StreamStopReason | null;
   sub_questions?: SubQuestionDetail[] | null;
   is_agentic?: boolean | null;
+  token_usage?: {
+    prompt_tokens: number;
+    completion_tokens: number;
+    total_tokens: number;
+    completion_tokens_details?: {
+      reasoning_tokens?: number;
+    };
+  } | null;
 
   // Streaming only
   second_level_generating?: boolean;
@@ -157,6 +165,14 @@ export interface BackendMessage {
   parentMessageId: number | null;
   refined_answer_improvement: boolean | null;
   is_agentic: boolean | null;
+  token_usage: {
+    prompt_tokens: number;
+    completion_tokens: number;
+    total_tokens: number;
+    completion_tokens_details?: {
+      reasoning_tokens?: number;
+    };
+  } | null;
 }
 
 export interface MessageResponseIDInfo {
@@ -177,6 +193,7 @@ export interface UserKnowledgeFilePacket {
   user_files: FileDescriptor[];
 }
 
+
 export interface DocumentsResponse {
   top_documents: OnyxDocument[];
   rephrased_query: string | null;
diff --git a/web/src/app/chat/lib.tsx b/web/src/app/chat/lib.tsx
index 6e89f56ee..b4719e298 100644
--- a/web/src/app/chat/lib.tsx
+++ b/web/src/app/chat/lib.tsx
@@ -137,7 +137,8 @@ export const isPacketType = (data: any): data is PacketType => {
     data.hasOwnProperty("message_id") ||
     data.hasOwnProperty("stop_reason") ||
     data.hasOwnProperty("user_message_id") ||
-    data.hasOwnProperty("reserved_assistant_message_id")
+    data.hasOwnProperty("reserved_assistant_message_id") ||
+    data.hasOwnProperty("usage")
   );
 };
 
@@ -523,6 +524,7 @@ export function processRawChatHistory(
       isImprovement:
         (messageInfo.refined_answer_improvement as unknown as boolean) || false,
       is_agentic: messageInfo.is_agentic,
+      token_usage: messageInfo.token_usage,
     };
 
     messages.set(messageInfo.message_id, message);
diff --git a/web/src/components/chat/TokenCounter.tsx b/web/src/components/chat/TokenCounter.tsx
new file mode 100644
index 000000000..135c9fb29
--- /dev/null
+++ b/web/src/components/chat/TokenCounter.tsx
@@ -0,0 +1,100 @@
+import React from 'react';
+import { SessionTokenUsage } from '@/hooks/useTokenCounter';
+import {
+  Tooltip,
+  TooltipContent,
+  TooltipProvider,
+  TooltipTrigger,
+} from "@/components/ui/tooltip";
+import { getDisplayNameForModel } from "@/lib/hooks";
+
+interface TokenCounterProps {
+  sessionUsage: SessionTokenUsage;
+  currentModel?: {
+    modelName: string;
+    maxTokens: number;
+  };
+}
+
+export const TokenCounter: React.FC<TokenCounterProps> = ({ 
+  sessionUsage, 
+  currentModel 
+}) => {
+  if (sessionUsage.contextTotalTokens === 0) {
+    return null;
+  }
+
+  const formatNumber = (num: number) => {
+    if (num >= 1000000) {
+      return (num / 1000000).toFixed(1) + 'M';
+    }
+    if (num >= 1000) {
+      return (num / 1000).toFixed(1) + 'k';
+    }
+    return num.toString();
+  };
+
+  // Calculate percentage based on context tokens (context window usage)
+  const maxTokens = currentModel?.maxTokens || 200000; // Default fallback
+  const tokenPercentage = (sessionUsage.contextTotalTokens / maxTokens) * 100;
+  
+  // Calculate total billed tokens
+  const totalBilledTokens = sessionUsage.billedPromptTokens + sessionUsage.billedCompletionTokens + sessionUsage.billedReasoningTokens;
+
+  return (
+    <TooltipProvider>
+      <Tooltip delayDuration={0}>
+        <TooltipTrigger asChild>
+          <div className="relative w-32 h-5 bg-neutral-200 dark:bg-neutral-700 rounded-full overflow-hidden">
+            <div
+              className={`absolute top-0 left-0 h-full rounded-full ${
+                tokenPercentage >= 100
+                  ? "bg-red-500 dark:bg-red-600"
+                  : tokenPercentage >= 80
+                  ? "bg-yellow-500 dark:bg-yellow-600"
+                  : "bg-green-500 dark:bg-green-600"
+              }`}
+              style={{
+                width: `${Math.min(tokenPercentage, 100)}%`,
+              }}
+            ></div>
+            {/* Text overlay */}
+            <div className="absolute inset-0 flex items-center justify-center">
+              <div className="flex items-center gap-1 text-xs font-medium whitespace-nowrap">
+                <span className="text-neutral-700 dark:text-neutral-200 drop-shadow-sm">
+                  {formatNumber(sessionUsage.contextTotalTokens)} / {formatNumber(maxTokens)}
+                </span>
+                <span className="text-neutral-500 dark:text-neutral-300 text-[10px] drop-shadow-sm">
+                  tokens
+                </span>
+              </div>
+            </div>
+          </div>
+        </TooltipTrigger>
+        <TooltipContent className="text-xs max-w-xs">
+          <div className="space-y-1">
+            <div className="font-medium">Conversation Token Usage</div>
+            <div className="space-y-1">
+              <div className="text-neutral-600 dark:text-neutral-300 font-medium">Billed Tokens:</div>
+              <div>Prompt: {sessionUsage.billedPromptTokens.toLocaleString()}</div>
+              <div>Completion: {sessionUsage.billedCompletionTokens.toLocaleString()}</div>
+              {sessionUsage.billedReasoningTokens > 0 && (
+                <div>Reasoning: {sessionUsage.billedReasoningTokens.toLocaleString()}</div>
+              )}
+            </div>
+            <div className="border-t pt-1 mt-2">
+              <div className="text-neutral-600 dark:text-neutral-300 font-medium">Context Usage:</div>
+              <div>Current: {sessionUsage.contextTotalTokens.toLocaleString()}</div>
+              <div>Max: {maxTokens.toLocaleString()} ({tokenPercentage.toFixed(1)}% used)</div>
+            </div>
+            {currentModel && (
+              <div className="text-neutral-400 mt-1">
+                Model: {getDisplayNameForModel(currentModel.modelName)}
+              </div>
+            )}
+          </div>
+        </TooltipContent>
+      </Tooltip>
+    </TooltipProvider>
+  );
+};
\ No newline at end of file
diff --git a/web/src/hooks/useTokenCounter.ts b/web/src/hooks/useTokenCounter.ts
new file mode 100644
index 000000000..4fc5cefee
--- /dev/null
+++ b/web/src/hooks/useTokenCounter.ts
@@ -0,0 +1,71 @@
+import { useState, useCallback } from 'react';
+
+export interface TokenUsage {
+  prompt_tokens: number;
+  completion_tokens: number;
+  total_tokens: number;
+  completion_tokens_details?: {
+    reasoning_tokens?: number;
+  };
+}
+
+export interface SessionTokenUsage {
+  // Billed tokens - incremental sum of each message (what you actually pay for)
+  billedPromptTokens: number;
+  billedCompletionTokens: number;
+  billedReasoningTokens: number;
+  
+  // Context tokens - current total context size (context window usage)
+  contextTotalTokens: number;
+  
+  messageCount: number;
+}
+
+export const useTokenCounter = () => {
+  const [sessionUsage, setSessionUsage] = useState<SessionTokenUsage>({
+    billedPromptTokens: 0,
+    billedCompletionTokens: 0,
+    billedReasoningTokens: 0,
+    contextTotalTokens: 0,
+    messageCount: 0,
+  });
+
+  const updateTokenUsage = useCallback((usage: TokenUsage) => {
+    setSessionUsage(prev => {
+      // For incremental billing:
+      // - First message: all prompt tokens are new
+      // - Subsequent messages: current prompt tokens - previous total context (since previous total = previous prompt + previous completion)
+      const incrementalPromptTokens = prev.messageCount === 0 
+        ? usage.prompt_tokens  // First message: all prompt tokens are new
+        : usage.prompt_tokens - prev.contextTotalTokens; // Subsequent: new prompt content only
+      
+      return {
+        // Billed tokens - add incremental costs
+        billedPromptTokens: prev.billedPromptTokens + Math.max(0, incrementalPromptTokens),
+        billedCompletionTokens: prev.billedCompletionTokens + usage.completion_tokens,
+        billedReasoningTokens: prev.billedReasoningTokens + (usage.completion_tokens_details?.reasoning_tokens || 0),
+        
+        // Context tokens - current total context size  
+        contextTotalTokens: usage.total_tokens,
+        
+        messageCount: prev.messageCount + 1,
+      };
+    });
+  }, []);
+
+  const resetTokenCounter = useCallback(() => {
+    setSessionUsage({
+      billedPromptTokens: 0,
+      billedCompletionTokens: 0,
+      billedReasoningTokens: 0,
+      contextTotalTokens: 0,
+      messageCount: 0,
+    });
+  }, []);
+
+  return {
+    sessionUsage,
+    updateTokenUsage,
+    resetTokenCounter,
+  };
+};
\ No newline at end of file
-- 
2.48.1

